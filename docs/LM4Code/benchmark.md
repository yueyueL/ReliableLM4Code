---
layout: default
title: Benchmark
parent: LM4Code_LM4SE
---
# Benchmarks for LM4Code/LM4SE
{: .no_toc }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

This page lists popular benchmarks for evaluating language models for code (LM4Code) and language models for software engineering (LM4SE).

### Relevant papers

### CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation
- Release year: 2021-02
- [Paper](https://arxiv.org/abs/2102.04664)
- [Repository](https://github.com/microsoft/CodeXGLUE)
- Description: Proposes a benchmark with 8 tasks to evaluate code understanding and generation models like CodeBERT, CodeGPT, GraphCodeBERT, etc.

### On the Evaluation of Neural Code Translation: Taxonomy and Benchmark
- Release year: 2023
- [Paper](https://arxiv.org/abs/2308.08961)
- Description: This paper proposes a taxonomy of neural code translation tasks and introduces a new benchmark, G-TransEval, for evaluating neural code translation models. G-TransEval includes a variety of code translation tasks with different levels of difficulty and complexity.

### Evaluating large language models trained on code
- Release year: 2021
- [Paper](https://arxiv.org/abs/2107.03374)
- Description: Evaluates Codex, AlphaCode, and other models on code translation, code completion, code summarization, and other tasks.

### A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
- Release year: 2023
- [Paper](https://arxiv.org/abs/2311.10372)


## Bug Repair
### Defects4J
- Release year: 2014
- Paper: ["Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs"](https://dl.acm.org/doi/10.1145/2610384.2628055)

### ManyBugs/IntroClass
- Release year: 2015
- Paper: ["The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs"](https://ieeexplore.ieee.org/document/7153570)

### BugAID
- Release year: 2016
- Paper: ["Discovering Bug Patterns in JavaScript"](https://dl.acm.org/doi/10.1145/2950290.2950308)

### CoCoNut
- Release year: 2020
- Paper: ["CoCoNuT: combining context-aware neural translation models using ensemble for program repair"](https://dl.acm.org/doi/10.1145/3395363.3397369)

### QuixBugs
- Release year: 2017
- Paper: ["QuixBugs: a multi-lingual program repair benchmark set based on the quixey challenge"](https://dl.acm.org/doi/10.1145/3135932.3135941)

### Bugs.jar
- Release year: 2018
- Paper: ["Bugs.jar: a large-scale, diverse dataset of real-world Java bugs"](https://dl.acm.org/doi/10.1145/3196398.3196473)

### BugsInPy
- Release year: 2020
- Paper: ["BugsInPy: A Database of Existing Bugs in Python Programs to Enable Controlled Testing and Debugging Studies"](https://dl.acm.org/doi/abs/10.1145/3368089.3417943)

### DeepFix
- Release year: 2017
- Paper: ["DeepFix: Fixing Common C Language Errors by Deep Learning"](https://ojs.aaai.org/index.php/AAAI/article/view/10742)


## Code Generation/Synthesis

### CONCODE
- Release year: 2018
- Paper: ["Mapping Language to Code in Programmatic Context"](https://arxiv.org/abs/1808.09588)

### HumanEval
- Release year: 2021
- Paper: ["Evaluating Large Language Models Trained on Code"](https://arxiv.org/abs/2107.03374) 

### MBPP/MathQA-Python
- Release year: 2021
- Paper: ["Program Synthesis with Large Language Models"](https://arxiv.org/abs/2108.07732) 

## Code Sumarization
### CODE-NN
- Release year: 2016
- Paper: ["Summarizing Source Code using a Neural Attention Model"](https://aclanthology.org/P16-1195/)

### 2018-07 IJCAI 2018 TL-CodeSum
- Release year: 2018
- Paper: ["Summarizing Source Code with Transferred API Knowledge"](https://www.ijcai.org/proceedings/2018/314)

### CodeSearchNet
- Release year: 2019
- Paper: ["CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"](https://arxiv.org/abs/1909.09436)